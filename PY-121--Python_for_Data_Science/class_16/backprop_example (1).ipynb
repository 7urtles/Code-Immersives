{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backprop_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bV5iR44Y_CJ"
      },
      "source": [
        "## Calculating gradients in three ways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9tCirK536t"
      },
      "source": [
        "Using the example from Chris Olah's [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/), you'll calcuate gradients for the function `f(a, b) = (a + b) * (b + 1)` in three ways:\n",
        "\n",
        "1. Numerically (nudge each parameter a little). Simple, fun, slow, a good way to check your work.\n",
        "2. Analytically (by hand , remembering our rules from calculus). Efficient, error prone.\n",
        "3. Backprop (reverse mode autodiff). Efficient, automatic.\n",
        "\n",
        "### Optional exercise\n",
        "In addition to Chris's article, I recommend this helpful [video](https://www.youtube.com/watch?v=d14TUNcbn1k) from Stanford's cs231 - one of the clearest explanations of backprop on the web (I went through a *ton* of them. If you find something clearer, please let me know so I can share with future students).\n",
        "\n",
        "As an exercise, try to calculate the gradients for one of the examples in that video using these methods, and verify you can get the same result with each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZs4NzdTRr_s"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96R1t8fgK_EN"
      },
      "source": [
        "## First example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSC_tWl-TFN"
      },
      "source": [
        "Our first function will be `f(a, b) = (a + b) * (b + 1)`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I80fotQCK2mW"
      },
      "source": [
        "### Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6GqLcCIo1S"
      },
      "source": [
        "def forward(a, b):\n",
        "  c = a + b # intermediate variables\n",
        "  d = b + 1\n",
        "  f = c * d\n",
        "  return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnFrL5YrXCmo"
      },
      "source": [
        "forward(2,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNK-xN7FLDI-"
      },
      "source": [
        "### Numeric gradient\n",
        "Useful to check your work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kXRmBCvI-3B"
      },
      "source": [
        "def numeric_gradient(my_function, params, h=1e-4):\n",
        "  \n",
        "  # Vector of partial derivatives\n",
        "  grads = []\n",
        "  \n",
        "  # Nudge each parameter by h, \n",
        "  # calculate how much the function changes\n",
        "  for i in range(len(params)):\n",
        "    \n",
        "    orginal_val = params[i]\n",
        "    \n",
        "    # f(x + h)\n",
        "    params[i] = orginal_val + h\n",
        "    plus_h = my_function(*params) \n",
        "        \n",
        "    # f(x - h)\n",
        "    params[i] = orginal_val - h\n",
        "    minus_h = my_function(*params)\n",
        "        \n",
        "    # Partial derivative\n",
        "    grad = (plus_h - minus_h) / (2 * h)\n",
        "    grads.append(grad)\n",
        "\n",
        "    # reset\n",
        "    params[i] = orginal_val\n",
        "    \n",
        "  return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PWzE2UWI4if"
      },
      "source": [
        "da, db = numeric_gradient(my_function=forward, params=[2, 1])\n",
        "print (\"Numeric gradient. da %0.2f, db %0.2f\" % (da, db))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MEhR5pfLLkC"
      },
      "source": [
        "### By hand\n",
        "Remembering our rules from calculus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVPs3gwCLOw5"
      },
      "source": [
        "def by_hand(a, b):\n",
        "  \n",
        "  # forward pass, keeping track of intermediate variables\n",
        "  c = a + b\n",
        "  d = b + 1\n",
        "  f = c * d\n",
        "  \n",
        "  df = 1 # Gradient of f on itself is 1 (would be replaced by loss)\n",
        "  dc = d # Product rule\n",
        "  dd = c # Product rule\n",
        "\n",
        "  dc_da = 1 # Sum rule\n",
        "  da = dc * dc_da\n",
        "  \n",
        "  dc_db = 1 # Sum rule\n",
        "  db_path_1 = dc * dc_db\n",
        "  \n",
        "  dd_db = 1 # Sum rule\n",
        "  db_path_2 = dd * dd_db\n",
        "  \n",
        "  db = db_path_1 + db_path_2 # Sum over paths (multivariate chain rule)\n",
        "  \n",
        "  return da, db\n",
        "\n",
        "da, db = by_hand(a=2, b=1)\n",
        "print (\"Gradient by backprop. da %0.2f, db %0.2f\" % (da, db))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVBlIAc8LzJD"
      },
      "source": [
        "### Autodiff\n",
        "Reverse mode autodiff using TensorFlow.\n",
        "\n",
        "Using a GradientTape ([doc](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape), [short tutorial](https://www.tensorflow.org/tutorials/customization/autodiff))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj18HaqQJAbk"
      },
      "source": [
        "params = [tf.Variable(2.0), tf.Variable(1.0)]\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  result = forward(*params)\n",
        "grads = tape.gradient(result, params)\n",
        "\n",
        "print(\"Autodiff. da %0.2f, db %0.2f\" % (grads[0], grads[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qMiGDvPQ-nk"
      },
      "source": [
        "## Second example\n",
        "\n",
        "Here's `f(x, y, z) = (x + y) * z` in the same three ways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAfpNI12RM8K"
      },
      "source": [
        "def forward(x, y, z):\n",
        "  return (x + y) * z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jaobiCEYX8n"
      },
      "source": [
        "forward(x=1, y=2, z=-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnuTOZ8gR4bo"
      },
      "source": [
        "### Numeric "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LX3YK8mR8fr"
      },
      "source": [
        "dx, dy, dz = numeric_gradient(my_function=forward, params=[1, 2, -4])\n",
        "print (\"Numeric gradient. dx %0.2f, dy %0.2f, dz %0.2f\" % (dx, dy, dz))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zb-YFTRvQ6"
      },
      "source": [
        "### Analytic gradient (by hand)\n",
        "Backprop by hand, if we rememeber our calculus rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P5HWOdx5yfO"
      },
      "source": [
        "def by_hand(x, y, z):\n",
        "  \n",
        "  # Calculate the forward pass, creating intermediate variables as we go.\n",
        "  q = x + y\n",
        "  f = q * z\n",
        "  \n",
        "  # Propagate the gradient on the function's output \n",
        "  # backward through intermediate variables, until we reach the inputs.\n",
        "  \n",
        "  # Gradient on the function is just itself. In a model, this would \n",
        "  # be replaced by the loss.\n",
        "  df = 1 \n",
        "  \n",
        "  # By product rule.\n",
        "  dq = z\n",
        "  \n",
        "  # By product rule.\n",
        "  dz = q\n",
        "  \n",
        "  dq_dx = 1 # By sum rule\n",
        "  dq_dy = 1 # By sum rule\n",
        "  \n",
        "  # Backprop gradient on q into x\n",
        "  dx = dq * dq_dx\n",
        "  \n",
        "  # Backprop gradient on q into y\n",
        "  dy = dq * dq_dy\n",
        "    \n",
        "  return dx, dy, dz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6c8yzX8Rmbn"
      },
      "source": [
        "dx, dy, dz = by_hand(1., 2., -4.)\n",
        "print (\"By hand. dx %0.2f, dy %0.2f, dz %0.2f\" % (dx, dy, dz))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uNh7itnSYLj"
      },
      "source": [
        "### Autodiff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0gsYnKEScTs"
      },
      "source": [
        "params = [tf.Variable(1.0), tf.Variable(2.0), tf.Variable(-4.0)]\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  result = forward(*params)\n",
        "grads = tape.gradient(result, params)\n",
        "\n",
        "print(\"Autodiff. dx %0.2f, dy %0.2f dz %0.2f\" % \n",
        "      (grads[0].numpy(), grads[1].numpy(), grads[2].numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sadXGr2_SR9N"
      },
      "source": [
        "## A matrix example\n",
        "\n",
        "`f(W, x) = ReLU(Wx)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL2PFlo5XW8Q"
      },
      "source": [
        "def forward(W, x):\n",
        "    wx = tf.matmul(W,x)\n",
        "    result = tf.nn.relu(wx)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg9c25M-XYhr"
      },
      "source": [
        "W = np.array([[-0.5, 0.2],\n",
        "              [-0.3, 0.8]])                  \n",
        "x = np.array([0.2, 0.4])  \n",
        "x = np.expand_dims(x,1)   \n",
        "forward(W,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srouRWQCZEm5"
      },
      "source": [
        "### Numeric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2geWmTqokYb"
      },
      "source": [
        "# Updated code to support matmul\n",
        "def numeric_gradient_v2(my_function, weights, inputs, h=1e-4):\n",
        "  \n",
        "  # Partials for each weight\n",
        "  grads = np.zeros_like(weights)\n",
        "  \n",
        "  # Written for clarity, not speed\n",
        "  for i in range(grads.shape[0]):\n",
        "    for j in range(grads.shape[1]):\n",
        "      \n",
        "      original = weights[i,j]\n",
        "      \n",
        "      weights[i,j] = original + h\n",
        "      plus_h = my_function(weights, inputs)\n",
        "      \n",
        "      weights[i,j] = original - h\n",
        "      minus_h = my_function(weights, inputs)\n",
        "      \n",
        "      # partial\n",
        "      grads[i,j] = np.sum(plus_h - minus_h) / (2 * h)\n",
        "      \n",
        "      # reset\n",
        "      weights[i,j] = original\n",
        "      \n",
        "  return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x88WSpuk0Nci"
      },
      "source": [
        "dw = numeric_gradient_v2(forward, weights=W, inputs=x)\n",
        "print (\"dw\\n\", dw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcxytV9BZLd7"
      },
      "source": [
        "### By hand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWyyOJef0Sol"
      },
      "source": [
        "def by_hand(W, x):\n",
        "  q = np.maximum(0, np.matmul(W, x))\n",
        "  dq = np.ones_like(q)\n",
        "  dq[q <= 0] = 0  \n",
        "  dw = np.matmul(dq, x.T)\n",
        "  return dw\n",
        "\n",
        "dw = by_hand(W,x)\n",
        "print (\"By hand\\n\", dw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBKP_eS7ZMf5"
      },
      "source": [
        "### Autodiff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-m03wlhztHT"
      },
      "source": [
        "W = tf.Variable([[-0.5, 0.2],\n",
        "                 [-0.3, 0.8]])                  \n",
        "x = tf.Variable([0.2, 0.4])  \n",
        "x = tf.expand_dims(x,1)   \n",
        "params = [W, x]\n",
        "with tf.GradientTape() as tape:\n",
        "  result = forward(*params)\n",
        "grads = tape.gradient(result, W)\n",
        "\n",
        "print(\"Autodiff. dw\", grads)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}