{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tL1kFYN_Oeb"
      },
      "source": [
        "# Tensorflow Project Exercise\n",
        "\n",
        "We'll use the [Bank Authentication Data Set](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) from the UCI repository.\n",
        "\n",
        "The data consists of 5 columns:\n",
        "\n",
        "* variance of Wavelet Transformed image (continuous)\n",
        "* skewness of Wavelet Transformed image (continuous)\n",
        "* curtosis of Wavelet Transformed image (continuous)\n",
        "* entropy of image (continuous)\n",
        "* class (integer)\n",
        "\n",
        "Where class indicates whether or not a Bank Note was authentic.\n",
        "\n",
        "This sort of task is perfectly suited for Neural Networks and Deep Learning! Just follow the instructions below to get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX6Q2yOu_Oec"
      },
      "source": [
        "## Get the Data\n",
        "\n",
        "*Use pandas to read in the data_banknote_authentication.txt file*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryadfz16_Oed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZcisNPq_Oeg"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data_banknote_authentication.txt')\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiuGt0tO_Oej"
      },
      "source": [
        "*Check the head of the Data*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NavHOhtV_Oek"
      },
      "outputs": [],
      "source": [
        "data.head()\n",
        "data['Class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dmzGHKT_Oen"
      },
      "source": [
        "## EDA\n",
        "\n",
        "We'll just do a few quick plots of the data.\n",
        "\n",
        "*Import seaborn and set matplolib inline for viewing*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIKfmL6C_Oeo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t6DFgTS_Oeq"
      },
      "source": [
        "*Create a Countplot of the Classes (Authentic 1 vs Fake 0)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXvN37iL_Oer"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data['Class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMZV02I3_Oeu"
      },
      "source": [
        "*Create a PairPlot of the Data with Seaborn, set Hue to Class*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vlj6qFgI_Oev"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data, hue='Class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJCtAWG2_Oex"
      },
      "source": [
        "## Data Preparation \n",
        "\n",
        "When using Neural Network and Deep Learning based systems, it is usually a good idea to Standardize your data, this step isn't actually necessary for our particular data set, but let's run through it for practice!\n",
        "\n",
        "### Standard Scaling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Djsa2lJ_Oey"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU88eYynB68x"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymd2kJI2_Oe0"
      },
      "source": [
        "**Create a StandardScaler() object called scaler.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppQjnBtc_Oe1"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z94zbtry_Oe3"
      },
      "source": [
        "**Fit scaler to the features.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukyuTUmN_Oe4"
      },
      "outputs": [],
      "source": [
        "scaler.fit(data.drop(['Class'], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rf85o7K_Oe6"
      },
      "source": [
        "**Use the .transform() method to transform the features to a scaled version.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KspsPUG_Oe7"
      },
      "outputs": [],
      "source": [
        "scaled_data = scaler.transform(data.drop(['Class'], axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AZaiaCh_Oe9"
      },
      "source": [
        "**Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vi5Lc8w_Oe-"
      },
      "outputs": [],
      "source": [
        "data_scaled = pd.DataFrame(data=scaled_data, columns=data.drop(['Class'], axis=1).columns)\n",
        "data_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32xmOO4_OfA"
      },
      "source": [
        "## Train Test Split\n",
        "\n",
        "*Create two objects X and y which are the scaled feature values and labels respectively.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jNArv38_OfC"
      },
      "outputs": [],
      "source": [
        "X = data_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDoIZH4v_OfE"
      },
      "outputs": [],
      "source": [
        "y = data['Class']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U3kwBuO_OfG"
      },
      "source": [
        "*Use SciKit Learn to create training and testing sets of the data:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkKIiiej_OfH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E_6B6m2_OfJ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onxfxPqz_OfL"
      },
      "source": [
        "# Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Dd4Ujk_OfL"
      },
      "outputs": [],
      "source": [
        "feat_col = []\n",
        "for col in data_scaled.columns:\n",
        "  feat_col.append(tf.feature_column.numeric_column(col))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcnKNO9x_OfO"
      },
      "source": [
        "*Create a list of feature column objects using tf.feature.numeric_column()*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpvpzYEX_OfO"
      },
      "outputs": [],
      "source": [
        "feat_col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlFGAslC_OfW"
      },
      "source": [
        "*Create an object called classifier which is a DNNClassifier from learn. Set it to have 2 classes and a [10,20,10] hidden unit layer structure:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKY4P9_q_OfX"
      },
      "outputs": [],
      "source": [
        "classifier = tf.estimator.DNNClassifier(hidden_units=[10,20,10], feature_columns=feat_col, n_classes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Uk2YLV_OfZ"
      },
      "source": [
        "*Now create a tf.estimator.pandas_input_fn that takes in your X_train, y_train, batch_size and set shuffle=True. You can play around with the batch_size parameter if you want, but let's start by setting it to 20 since our data isn't very big.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_wvJk_a_OfZ"
      },
      "outputs": [],
      "source": [
        "input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(X_train, y_train, batch_size=20, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPi6Z_Eh_Ofc"
      },
      "source": [
        "*Now train classifier to the input function. Use steps=500. You can play around with these values if you want!*\n",
        "\n",
        "*Note: Ignore any warnings you get, they won't effect your output*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3ejryAQ_Ofc"
      },
      "outputs": [],
      "source": [
        "classifier.train(input_fn, steps=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUPGENzm_Ofe"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_oWTb7m_Off"
      },
      "source": [
        "*Create another pandas_input_fn that takes in the X_test data for x. Remember this one won't need any y_test info since we will be using this for the network to create its own predictions. Set shuffle=False since we don't need to shuffle for predictions.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90NcVGjs_Off"
      },
      "outputs": [],
      "source": [
        "pred_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(X_test, batch_size=len(X_test), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi72J9mv_Ofi"
      },
      "source": [
        "*Use the predict method from the classifier model to create predictions from X_test*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSq2g82C_Ofj"
      },
      "outputs": [],
      "source": [
        "predictions = list(classifier.predict(pred_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYLg4ivu_Ofl"
      },
      "outputs": [],
      "source": [
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQt_GgHG_Ofn"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for pred in predictions:\n",
        "  results.append(pred['class_ids'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGK5ZDA__Ofp"
      },
      "source": [
        "*Now create a classification report and a Confusion Matrix. Does anything stand out to you?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtKjp7H4_Ofq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehTr_wnh_Ofs"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(y_test, results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUrohYcV_Ofu",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRejYPRH_Ofv"
      },
      "source": [
        "## Optional Comparison\n",
        "\n",
        "*You should have noticed extremely accurate results from the DNN model. Let's compare this to a Random Forest Classifier for a reality check!*\n",
        "\n",
        "*Use SciKit Learn to Create a Random Forest Classifier and compare the confusion matrix and classification report to the DNN model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG_KqqQP_Ofw"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv2gkORq_Ofy"
      },
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsOOXEV3_Of0"
      },
      "outputs": [],
      "source": [
        "rfc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klRCcZBS_Of2"
      },
      "outputs": [],
      "source": [
        "rfc_pred = rfc.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7il5aLg2_Of4"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, rfc_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QehWLlh_Of7"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(y_test, rfc_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMlRih9u_Of-"
      },
      "source": [
        "*It should have also done very well, possibly perfect! Hopefully you have seen the power of DNN!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPnwITbb_Of-"
      },
      "source": [
        "# Cheers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi0Ulz9oA4t_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tensorflow_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
